{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1165040,"sourceType":"datasetVersion","datasetId":660021,"isSourceIdPinned":false}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os \nimport numpy as np\nfrom tqdm.auto import tqdm\nimport cv2 as cv\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T19:47:14.033205Z","iopub.execute_input":"2025-06-05T19:47:14.033987Z","iopub.status.idle":"2025-06-05T19:47:15.854902Z","shell.execute_reply.started":"2025-06-05T19:47:14.033963Z","shell.execute_reply":"2025-06-05T19:47:15.854224Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torchvision\nimport torchvision.transforms.v2 as transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nfrom albumentations import HorizontalFlip, VerticalFlip, Rotate\nfrom torch.autograd import detect_anomaly\nfrom torch.utils.tensorboard import SummaryWriter\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T19:47:15.856231Z","iopub.execute_input":"2025-06-05T19:47:15.856644Z","iopub.status.idle":"2025-06-05T19:47:52.411787Z","shell.execute_reply.started":"2025-06-05T19:47:15.856614Z","shell.execute_reply":"2025-06-05T19:47:52.411071Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T19:47:52.412557Z","iopub.execute_input":"2025-06-05T19:47:52.413529Z","iopub.status.idle":"2025-06-05T19:47:52.416967Z","shell.execute_reply.started":"2025-06-05T19:47:52.413508Z","shell.execute_reply":"2025-06-05T19:47:52.416142Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"franciscoescobar/satellite-images-of-water-bodies\")\n\nprint(\"Path to dataset files:\", path)\n\nimg_folder = path+\"/Water Bodies Dataset/Images\"\nmask_folder = path+\"/Water Bodies Dataset/Masks\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T19:47:52.418918Z","iopub.execute_input":"2025-06-05T19:47:52.419265Z","iopub.status.idle":"2025-06-05T19:47:52.658755Z","shell.execute_reply.started":"2025-06-05T19:47:52.419238Z","shell.execute_reply":"2025-06-05T19:47:52.657887Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images_name = sorted(os.listdir(img_folder))\nmasks_name = sorted(os.listdir(mask_folder))\n\nimage_files_list  = [os.path.join(img_folder,img_name) for img_name in images_name]\nmask_files_list  = [os.path.join(mask_folder,mask_name) for mask_name in masks_name]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T19:47:52.659657Z","iopub.execute_input":"2025-06-05T19:47:52.659942Z","iopub.status.idle":"2025-06-05T19:47:52.754760Z","shell.execute_reply.started":"2025-06-05T19:47:52.659914Z","shell.execute_reply":"2025-06-05T19:47:52.754136Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## plotting some of the images\nfig,axs = plt.subplots(2,10, figsize= (50,10))\nax = axs.flatten()\nfor i in range(0,10):\n    ax[i].imshow(np.array(Image.open(image_files_list[i]).convert('RGB')))\n    ax[i+10].imshow(np.array(Image.open(mask_files_list[i])))\n    \nplt.tight_layout()\nplt.show()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T19:47:52.755495Z","iopub.execute_input":"2025-06-05T19:47:52.755714Z","iopub.status.idle":"2025-06-05T19:47:58.161193Z","shell.execute_reply.started":"2025-06-05T19:47:52.755697Z","shell.execute_reply":"2025-06-05T19:47:58.160296Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#checking the images mode\nimg = Image.open(image_files_list[15])\nimg.mode","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T19:47:58.162295Z","iopub.execute_input":"2025-06-05T19:47:58.162752Z","iopub.status.idle":"2025-06-05T19:47:58.175255Z","shell.execute_reply.started":"2025-06-05T19:47:58.162704Z","shell.execute_reply":"2025-06-05T19:47:58.174512Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#creating data class\n\nclass WaterDataset(Dataset):\n    def __init__ (self, img_list, mask_list, transform= None):\n        self.img_list = img_list\n        self.mask_list = mask_list\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.img_list)\n\n    def __getitem__(self, idx):\n        image_path, mask_path  = self.img_list[idx] , self.mask_list[idx]\n\n        img = np.array(Image.open(image_path),dtype=np.float32)\n        img = img/255.0 \n\n        mask = np.array(Image.open(mask_path).convert('L'),dtype=np.float32)\n        mask = mask/255.0\n        \n        # print(img.shape, mask.shape)\n        if self.transform :\n            transformed  = self.transform(image=img,mask = mask)\n            img = transformed['image']\n            mask = transformed['mask']\n\n        else:\n            \n            img , mask = torch.tensor(img).permute(2,0,1),torch.tensor(mask)\n            img = torchvision.transforms.functional.normalize(img,(0.5,0.5,0.5),(0.5,0.5,0.5))\n        \n        mask=mask.unsqueeze(0)\n        return img, mask\n\n\n            ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T19:47:58.176144Z","iopub.execute_input":"2025-06-05T19:47:58.176403Z","iopub.status.idle":"2025-06-05T19:47:58.183481Z","shell.execute_reply.started":"2025-06-05T19:47:58.176386Z","shell.execute_reply":"2025-06-05T19:47:58.182824Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transform= A.Compose([\n    A.Resize(256,256,1.0),\n    A.HorizontalFlip(),\n    \n    # transforms.ColorJitter(brightness=0.2,contrast=0.2),\n    # transforms.RandomAffine(degrees=15, scale=(0.95,1.05), translate=(0.05,0.05)),\n\n    # A.normalize input is (height,width,3) for RGB and (height,width,1) for greyscale\n    # different from torchvision.transforms.functional.noramlize()\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    A.pytorch.ToTensorV2()\n    \n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T19:47:58.184462Z","iopub.execute_input":"2025-06-05T19:47:58.184765Z","iopub.status.idle":"2025-06-05T19:47:58.200232Z","shell.execute_reply.started":"2025-06-05T19:47:58.184743Z","shell.execute_reply":"2025-06-05T19:47:58.199352Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from  sklearn.model_selection import train_test_split\nX_train , X_val , y_train, y_val = train_test_split(image_files_list, mask_files_list,test_size = 0.2 , random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T19:47:58.203337Z","iopub.execute_input":"2025-06-05T19:47:58.203635Z","iopub.status.idle":"2025-06-05T19:47:58.212609Z","shell.execute_reply.started":"2025-06-05T19:47:58.203609Z","shell.execute_reply":"2025-06-05T19:47:58.211867Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = WaterDataset (X_train,y_train,transform=transform)\ntest_dataset = WaterDataset (X_val,y_val,transform = transform)\n\ntrain_loader = DataLoader(train_dataset,batch_size= 32 , num_workers = 4 , shuffle = True)\ntest_loader = DataLoader(test_dataset, batch_size = 32 , num_workers =4 , shuffle = True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T19:47:58.213492Z","iopub.execute_input":"2025-06-05T19:47:58.213723Z","iopub.status.idle":"2025-06-05T19:47:58.228328Z","shell.execute_reply.started":"2025-06-05T19:47:58.213707Z","shell.execute_reply":"2025-06-05T19:47:58.227613Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# checking the shapes of the tensors\ncheck_img , check_mask = train_dataset[2]\n## check_img , check_mask = train_dataset[2]\ncheck_img.shape, check_mask.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T19:47:58.229236Z","iopub.execute_input":"2025-06-05T19:47:58.229766Z","iopub.status.idle":"2025-06-05T19:47:58.379534Z","shell.execute_reply.started":"2025-06-05T19:47:58.229742Z","shell.execute_reply":"2025-06-05T19:47:58.378845Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch import nn\n\nclass horizontal (nn.Module):\n    def __init__ (self, channel_in, channel_out):\n        super().__init__()\n        # ouput_size = [(input_size -kennel_size +2* padding)/ stride] +1\n        self.conv = nn.Sequential(\n            nn.Conv2d(channel_in,channel_out,3,padding=1),\n            nn.BatchNorm2d(channel_out),\n            nn.Conv2d(channel_out,channel_out,3,padding=1),\n            nn.BatchNorm2d(channel_out),\n            nn.LeakyReLU(negative_slope=0.01)\n            # LeakyReLU was used to reduce Vanishing Gradient\n            # nn.ReLU(inplace=True)\n        )\n\n    def forward (self, x):\n        return self.conv(x)\n\nclass downward (nn.Module):\n    def __init__ (self, channel_in , channel_out):\n        super().__init__()\n        self.down = nn.Sequential(\n            nn.MaxPool2d(2),\n            horizontal(channel_in,channel_out)\n        )\n    \n    def forward (self,x):\n        return self.down(x)\n\n\nclass upward (nn.Module):\n    def __init__ (self, channel_in,channel_out):\n        super().__init__()\n        # output = stride * (input_size -1) + Kernel_size() - 2*padding\n        self.up = nn.ConvTranspose2d(channel_in, channel_in //2 , 2, stride =2)\n        self.horizontal = horizontal(channel_in,channel_out)\n\n    def forward(self,x1, x2):\n        x1 = self.up(x1)\n        diff_y = x2.size()[2] - x1.size()[2]\n        diff_x = x2.size()[3] - x1.size()[3]\n        x1 = torch.nn.functional.pad(x1,[diff_x//2, diff_x -diff_x//2 , diff_y//2, diff_y-diff_y//2])\n        x2 = torch.cat([x2,x1], dim=1)\n\n        return self.horizontal(x2)\n\nclass OutputMap (nn.Module):\n    def __init__(self, channel_in,channel_out):\n        super().__init__()\n        self.out = nn.Sequential(\n            nn.Conv2d(channel_in,channel_out,1)#,\n            # nn.Sigmoid()\n        )\n\n    def forward(self,x):\n        return self.out(x)\n\n\n\n\nclass UNet (nn.Module):\n    def __init__ (self, n_channels , n_classes):\n        super().__init__()\n        self.horizontal1 = horizontal(n_channels,64)\n        self.down1 = downward(64,128)\n        self.down2 = downward(128,256)\n        self.down3 = downward(256,512)\n        self.down4 = downward(512,1024)\n        self.up1 = upward(1024,512)\n        self.up2 = upward(512,256)\n        self.up3 = upward(256,128)\n        self.up4 = upward(128,64)\n        self.outc = OutputMap (64, n_classes)\n\n    def forward(self,x):\n        x1 = self.horizontal1(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5,x4)\n        x = self.up2(x,x3)\n        x = self.up3(x,x2)\n        x = self.up4(x,x1)\n        logits = self.outc(x)\n\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T19:47:58.380333Z","iopub.execute_input":"2025-06-05T19:47:58.380664Z","iopub.status.idle":"2025-06-05T19:47:58.393746Z","shell.execute_reply.started":"2025-06-05T19:47:58.380637Z","shell.execute_reply":"2025-06-05T19:47:58.393141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # printing the summary to check for error\n# from torchsummary import summary\n# model = UNet(3,1)\n# summary(model,(3,256,256))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T19:47:58.394413Z","iopub.execute_input":"2025-06-05T19:47:58.394729Z","iopub.status.idle":"2025-06-05T19:47:58.416437Z","shell.execute_reply.started":"2025-06-05T19:47:58.394704Z","shell.execute_reply":"2025-06-05T19:47:58.415612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Using the sum of DiceLoss and BCELoss as the Loss function\n\n#dice_loss\ndef dice_loss (pred,target,epsilon=1e-6):\n\n    pred = torch.sigmoid(pred)\n       \n    intersection = (pred * target).sum()\n    dice = (2.0 * intersection + epsilon) / (pred.sum() + target.sum() + epsilon)\n    return 1 - dice\n\ndef jaccard_loss (pred,target,epsilon = 1e-6):\n\n    pred = torch.sigmoid(pred)\n\n    intersection = (pred * target).sum()\n    union = pred.sum() + target.sum() - intersection\n    iou = intersection / (union + epsilon)\n    return 1 - iou\n    \n\n#Creating combinedLoss \nclass CombinedLoss(nn.Module):\n    def __init__(self,bce_weight=0.6,beta=0.7):\n        super().__init__()\n\n        self.bce_loss= nn.BCEWithLogitsLoss()\n        # self.bce_loss= nn.BCELoss()\n\n        self.bce_weight = bce_weight\n    \n    def forward(self,logits,targets,return_individual=True):\n        \n        dice = dice_loss(logits,targets)\n        \n        bce = self.bce_loss(logits,targets)\n        # iou = jaccard_loss(logits,targets)\n        combined = (1-self.bce_weight)*dice + (self.bce_weight)*bce\n\n        if return_individual:\n            return combined, dice, bce\n        return combined\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T19:47:58.417247Z","iopub.execute_input":"2025-06-05T19:47:58.418372Z","iopub.status.idle":"2025-06-05T19:47:58.432939Z","shell.execute_reply.started":"2025-06-05T19:47:58.418353Z","shell.execute_reply":"2025-06-05T19:47:58.432280Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# checking for vanishing gradient\n# def plot_grad_flow(named_parameters):\n#     ave_grads = []\n#     layers = []\n#     for n, p in named_parameters:\n#         if(p.requires_grad) and (\"bias\" not in n):\n#             layers.append(n)\n#             ave_grads.append(p.grad.abs().mean().cpu())\n#             max_grads.append(p.grad.abs().max().cpu())\n    \n#     plt.plot(ave_grads, alpha=0.3, color=\"b\")\n#     plt.hlines(0, 0, len(ave_grads)+1, linewidth=1, color=\"k\" )\n#     plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n#     plt.xlim(xmin=0, xmax=len(ave_grads))\n#     plt.xlabel(\"Layers\")\n#     plt.ylabel(\"average gradient\")\n#     plt.title(\"Gradient flow\")\n#     plt.grid(True)\n#     plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T19:47:58.433803Z","iopub.execute_input":"2025-06-05T19:47:58.434340Z","iopub.status.idle":"2025-06-05T19:47:58.457135Z","shell.execute_reply.started":"2025-06-05T19:47:58.434312Z","shell.execute_reply":"2025-06-05T19:47:58.456609Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from matplotlib.lines import Line2D\ndef plot_grad_flow(named_parameters):\n    '''Plots the gradients flowing through different layers in the net during training.\n    Can be used for checking for possible gradient vanishing / exploding problems.\n    \n    Usage: Plug this function in Trainer class after loss.backwards() as \n    \"plot_grad_flow(self.model.named_parameters())\" to visualize the gradient flow'''\n    ave_grads = []\n    max_grads= []\n    layers = []\n    for n, p in named_parameters:\n        if(p.requires_grad) and (\"bias\" not in n):\n            layers.append(n)\n            ave_grads.append(p.grad.abs().mean().cpu())\n            max_grads.append(p.grad.abs().max().cpu())\n    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color=\"c\")\n    plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color=\"b\")\n    plt.hlines(0, 0, len(ave_grads)+1, lw=2, color=\"k\" )\n    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n    plt.xlim(left=0, right=len(ave_grads))\n    plt.ylim(bottom = -0.001, top=0.02) # zoom in on the lower gradient regions\n    plt.xlabel(\"Layers\")\n    plt.ylabel(\"average gradient\")\n    plt.title(\"Gradient flow\")\n    plt.grid(True)\n    plt.legend([Line2D([0], [0], color=\"c\", lw=4),\n                Line2D([0], [0], color=\"b\", lw=4),\n                Line2D([0], [0], color=\"k\", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T19:47:58.457723Z","iopub.execute_input":"2025-06-05T19:47:58.457952Z","iopub.status.idle":"2025-06-05T19:47:58.476645Z","shell.execute_reply.started":"2025-06-05T19:47:58.457938Z","shell.execute_reply":"2025-06-05T19:47:58.476064Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Training Function\ndef train(model, train_loader, val_loader, optimizer, criterion, epochs,scheduler=None):\n    train_losses, val_losses = [], []\n    train_dice_losses, val_dice_losses = [], []\n    train_bce_losses, val_bce_losses = [], []\n    \n    writer = SummaryWriter()\n    scaler = torch.amp.GradScaler('cuda')\n    \n    for epoch in tqdm(range(epochs),desc=\"Epoch\",position=0, leave=True):\n        \n        model.train()\n        # resetting the loss for each full batch propagations\n        total_training_loss ,total_training_dice ,total_training_bce = 0.0 , 0.0 , 0.0\n        \n        for  images, masks in tqdm(train_loader,desc=\"training_batch\",position=1, leave=True):\n            \n            images , masks = images.to(device) , masks.to(device)        \n\n            optimizer.zero_grad()\n            with torch.amp.autocast('cuda'):\n                output = model(images)\n                total_loss, dice_component, bce_component = criterion(output, masks, return_individual=True)\n            scaler.scale(total_loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            ### added to check if there is any nan or exploding gradient\n            # with detect_anomaly():\n            # logits = model(images)\n            # loss  = criterion(logits,masks)\n            # optimizer.zero_grad()\n            # loss.backward()\n            # optimizer.step()\n            \n            total_training_loss += total_loss.item()\n            total_training_dice += dice_component.item()\n            total_training_bce += bce_component.item()\n            \n        if epoch %2 ==0:\n            plot_sample_images(images,masks,model,epoch)\n                      \n        # gradient flow and check for vanishing gradient  \n        # if epoch <2:\n        #     plot_grad_flow(model.named_parameters())  \n        \n        # Calculate and append training loss each epoch\n        train_loss_this_epoch = total_training_loss / len(train_loader)\n        train_dice_this_epoch = total_training_dice / len(train_loader)\n        train_bce_this_epoch = total_training_bce / len(train_loader)\n        \n        train_losses.append(train_loss_this_epoch)\n        train_dice_losses.append(train_dice_this_epoch)\n        train_bce_losses.append(train_bce_this_epoch)\n        # writer.add_scalar(\"Loss/train\", train_loss_this_epoch, epoch)\n\n        if scheduler is not None:\n            scheduler.step()\n        \n        model.eval()\n        total_val_loss, total_val_dice, total_val_bce  = 0.0 , 0.0, 0.0\n        \n        with torch.no_grad():\n            for images,masks in tqdm(val_loader,desc=\"val_batch\",position=2, leave=True):\n                images, masks = images.to(device) , masks.to(device)\n                \n                logits = model(images)\n                total_loss, dice_component, bce_component = criterion(logits, masks, return_individual=True)\n                \n                total_val_loss += total_loss.item()\n                total_val_dice += dice_component.item()\n                total_val_bce += bce_component.item()\n                \n        if epoch %2 ==0:\n            plot_sample_images(images,masks,model,epoch)\n        \n        val_loss_this_epoch = total_val_loss / len(val_loader)\n        val_dice_this_epoch = total_val_dice / len(val_loader)\n        val_bce_this_epoch = total_val_bce / len(val_loader)\n        \n        val_losses.append(val_loss_this_epoch)\n        val_dice_losses.append(val_dice_this_epoch)\n        val_bce_losses.append(val_bce_this_epoch)\n\n        \n        # writer.add_scalar(\"Loss/test\", val_loss_this_epoch, epoch)\n\n        print(f'Epoch: {epoch+1}/{epochs}')\n        print(f'  Train - Total: {train_loss_this_epoch:.4f}, Dice: {train_dice_this_epoch:.4f}, BCE: {train_bce_this_epoch:.4f}')\n        print(f'  Val   - Total: {val_loss_this_epoch:.4f}, Dice: {val_dice_this_epoch:.4f}, BCE: {val_bce_this_epoch:.4f}')\n        print('-' * 80)\n        \n    # writer.close()\n        \n    return [train_losses , train_dice_losses,train_bce_losses], [val_losses ,val_dice_losses,val_bce_losses]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T19:47:58.477428Z","iopub.execute_input":"2025-06-05T19:47:58.478267Z","iopub.status.idle":"2025-06-05T19:47:58.500712Z","shell.execute_reply.started":"2025-06-05T19:47:58.478246Z","shell.execute_reply":"2025-06-05T19:47:58.499903Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_sample_images(images,masks,model,epoch):\n    fig, axes = plt.subplots(1, 3, figsize=(5, 3))\n    # fig.suptitle(f'Epoch {epoch + 1} - Detailed Predictions', fontsize=16)\n    \n    ax=axes.flatten()\n    img_np = images[3].cpu().permute(1,2,0)\n    mean = np.array([0.485, 0.456, 0.406]).reshape(1, 1, 3)\n    std = np.array([0.229, 0.224, 0.225]).reshape(1, 1, 3)\n    img_np = img_np * std + mean\n    \n    mask_np = masks[3].cpu().permute(1,2,0)\n    \n    with torch.no_grad():\n        logits = model(images[3].cpu().unsqueeze(0))\n        prediction = torch.sigmoid(logits[0])\n        #setting threshold as 0.5\n        prediction=(prediction > 0.5)*1.0\n    \n    \n    ax[0].imshow(img_np*255.0)\n    ax[1].imshow(mask_np*255.0, cmap='gray')\n    ax[2].imshow(prediction.permute(1,2,0).cpu().numpy()*255.0)\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T19:47:58.501495Z","iopub.execute_input":"2025-06-05T19:47:58.501787Z","iopub.status.idle":"2025-06-05T19:47:58.525225Z","shell.execute_reply.started":"2025-06-05T19:47:58.501768Z","shell.execute_reply":"2025-06-05T19:47:58.524363Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"''' Forward hook for testing if the output is correct'''\n# model = UNet(3,1)\n\n# # forward hook testing\n# def hook_fn(module , input, output):\n#     print(f\"Layer : {module} , Output Shape: {output.shape}\")\n\n# for layer in model.children():\n#     layer.register_forward_hook(hook_fn)\n\n# dummy_input = torch.randn(3,3,256,256)\n# model(dummy_input)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T16:43:55.342256Z","iopub.execute_input":"2025-06-05T16:43:55.342460Z","iopub.status.idle":"2025-06-05T16:43:55.347162Z","shell.execute_reply.started":"2025-06-05T16:43:55.342445Z","shell.execute_reply":"2025-06-05T16:43:55.346544Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### model getting stuck in suboptimal regions during the first few epochs of training with Adam","metadata":{}},{"cell_type":"code","source":"model = UNet(3,1)\nmodel = nn.DataParallel(model)\nmodel.to(device)\n\noptimizer = torch.optim.RAdam(model.parameters(), lr=0.001)  \ncriterion = CombinedLoss()\nepochs = 10\n\ntrain_losses, val_losses = train(model, train_loader, test_loader, optimizer,criterion,epochs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T11:12:15.591777Z","iopub.execute_input":"2025-05-31T11:12:15.592038Z","iopub.status.idle":"2025-05-31T11:28:06.391655Z","shell.execute_reply.started":"2025-05-31T11:12:15.592021Z","shell.execute_reply":"2025-05-31T11:28:06.390751Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T16:38:55.426423Z","iopub.execute_input":"2025-06-05T16:38:55.426693Z","iopub.status.idle":"2025-06-05T16:38:55.435371Z","shell.execute_reply.started":"2025-06-05T16:38:55.426674Z","shell.execute_reply":"2025-06-05T16:38:55.434806Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = UNet(3,1)\nmodel = nn.DataParallel(model)\nmodel.to(device)\n\n# using he initialisation to help with vanishing gradient\ndef he_init(layer):\n    if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.ConvTranspose2d):\n        nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='leaky_relu')\n    elif isinstance(layer, nn.BatchNorm2d):\n        nn.init.constant_(layer.weight, 1)\n        nn.init.constant_(layer.bias, 0)\n\nmodel.apply(he_init)\n\noptimizer = torch.optim.RAdam(model.parameters(), lr=0.00001, weight_decay=1e-3)\n# changed from 0.001 as it had shown some slower\n\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n\ncriterion = CombinedLoss()\nepochs = 30\n\ntrain_losses_list, val_losses_list = train(model, train_loader, test_loader, optimizer,criterion,epochs,scheduler)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T12:46:35.487641Z","iopub.execute_input":"2025-05-31T12:46:35.488380Z","iopub.status.idle":"2025-05-31T13:29:35.749076Z","shell.execute_reply.started":"2025-05-31T12:46:35.488358Z","shell.execute_reply":"2025-05-31T13:29:35.748088Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# torch.save(model.state_dict(),\"test_model.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T12:23:27.492413Z","iopub.execute_input":"2025-05-31T12:23:27.492694Z","iopub.status.idle":"2025-05-31T12:23:27.724491Z","shell.execute_reply.started":"2025-05-31T12:23:27.492675Z","shell.execute_reply":"2025-05-31T12:23:27.723934Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig,axs = plt.subplots(1,3,figsize=(10,3))\nax= axs.flatten()\ni=0\nfor train_losses,val_losses, label in zip (train_losses_list, val_losses_list,['combined_loss','bce_loss','dice_loss']):\n    ax[i].plot(np.arange(epochs), train_losses)\n    ax[i].plot(np.arange(epochs), val_losses)\n    ax[i].set_xlabel(\"Epoch\")\n    ax[i].set_ylabel(\"Loss\")\n    ax[i].legend([f\"Train {label}\", f\"Val {label}\"])\n    i+=1\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T13:29:35.751339Z","iopub.execute_input":"2025-05-31T13:29:35.751705Z","iopub.status.idle":"2025-05-31T13:29:36.076155Z","shell.execute_reply.started":"2025-05-31T13:29:35.751662Z","shell.execute_reply":"2025-05-31T13:29:36.075570Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.savefig(f'epoch_30_detailed_predictions.png', dpi=300, bbox_inches='tight')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T14:32:45.392435Z","iopub.execute_input":"2025-05-31T14:32:45.393171Z","iopub.status.idle":"2025-05-31T14:32:45.520009Z","shell.execute_reply.started":"2025-05-31T14:32:45.393144Z","shell.execute_reply":"2025-05-31T14:32:45.519270Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = UNet(3,1)\nmodel = nn.DataParallel(model)\nmodel.to(device)\n\n# using he initialisation to help with vanishing gradient\ndef he_init(layer):\n    if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.ConvTranspose2d):\n        nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='leaky_relu')\n    elif isinstance(layer, nn.BatchNorm2d):\n        nn.init.constant_(layer.weight, 1)\n        nn.init.constant_(layer.bias, 0)\n\nmodel.apply(he_init)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.00001, weight_decay=1e-3)\n# optimizer = torch.optim.RAdam(model.parameters(), lr=0.00001, weight_decay=1e-3)\n# changed from 0.001 as it had shown some slower\n\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=epochs//3, gamma=0.1 )\n\ncriterion = CombinedLoss()\nepochs = 30\n\ntrain_losses_list, val_losses_list = train(model, train_loader, test_loader, optimizer,criterion,epochs,scheduler)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T19:49:44.216013Z","iopub.execute_input":"2025-06-05T19:49:44.216754Z","iopub.status.idle":"2025-06-05T20:33:13.154263Z","shell.execute_reply.started":"2025-06-05T19:49:44.216724Z","shell.execute_reply":"2025-06-05T20:33:13.153288Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig,axs = plt.subplots(1,3,figsize=(10,3))\nax= axs.flatten()\ni=0\nfor train_losses,val_losses, label in zip (train_losses_list, val_losses_list,['combined_loss','bce_loss','dice_loss']):\n    ax[i].plot(np.arange(epochs), train_losses)\n    ax[i].plot(np.arange(epochs), val_losses)\n    ax[i].set_xlabel(\"Epoch\")\n    ax[i].set_ylabel(\"Loss\")\n    ax[i].legend([f\"Train {label}\", f\"Val {label}\"])\n    i+=1\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T20:33:13.156423Z","iopub.execute_input":"2025-06-05T20:33:13.156704Z","iopub.status.idle":"2025-06-05T20:33:13.550422Z","shell.execute_reply.started":"2025-06-05T20:33:13.156669Z","shell.execute_reply":"2025-06-05T20:33:13.549637Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fig,axs = plt.subplots(1,3,figsize=(10,3))\n# ax= axs.flatten()\n# i=0\n# for train_losses,val_losses, label in zip (train_losses_list, val_losses_list,['combined_loss','bce_loss','dice_loss']):\n#     ax[i].plot(np.arange(epochs), train_losses)\n#     ax[i].plot(np.arange(epochs), val_losses)\n#     ax[i].set_xlabel(\"Epoch\")\n#     ax[i].set_ylabel(\"Loss\")\n#     ax[i].legend([f\"Train {label}\", f\"Val {label}\"])\n#     i+=1\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T13:19:23.586777Z","iopub.execute_input":"2025-06-06T13:19:23.586948Z","iopub.status.idle":"2025-06-06T13:19:23.592011Z","shell.execute_reply.started":"2025-06-06T13:19:23.586932Z","shell.execute_reply":"2025-06-06T13:19:23.591204Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#things to try:\n#replacing all the expansion path conv blacks with DeenseNet\n1. changing all the conv block to desnse net\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T23:04:51.781308Z","iopub.execute_input":"2025-05-30T23:04:51.782052Z","iopub.status.idle":"2025-05-30T23:04:51.907312Z","shell.execute_reply.started":"2025-05-30T23:04:51.782025Z","shell.execute_reply":"2025-05-30T23:04:51.906422Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), 'vanilla_unet_model.pth')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r''vanilla_unet_model.pth')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# try replacing all the conv block in decoder to see if it helps ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## efficient dense unet\n\n#replacing all the expansion path conv blacks with DeenseNet\n\n#i.e. effiecnet dense U-net","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}